{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Named Entity Recognition (NER) from DOCX using Stanza\n",
        "\n",
        "This notebook extracts **person names** and **locations** from a `.docx` file using the **Stanza NLP library**.  \n",
        "It supports both **Ukrainian** and **Russian** text.\n",
        "\n",
        "---\n",
        "## Setup Instructions\n",
        "\n",
        "Before running the notebook, you'll need to install a few Python libraries and prepare your document.\n",
        "\n",
        "### Install Required Python Libraries\n",
        "\n",
        "You can install all the required packages by running the following code cell in the notebook:"
      ],
      "metadata": {
        "id": "xY7kbQyYO0Mk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install stanza python-docx torch==2.5.0 numpy"
      ],
      "metadata": {
        "id": "v9o6O79X4Wco"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 📁 Step 1: Uploading Files\n",
        "\n",
        "Before executing the next code cell, you must upload a `.docx` transcript that contains the text you want to analyze.\n",
        "\n",
        "To upload files in **Google Colab**:\n",
        "\n",
        "- Click on the **folder icon** labeled **Files** on the left sidebar.  \n",
        "- Click the **Upload icon** to upload your `.docx` file from your computer.  \n",
        "- Once uploaded, **right-click** the file name and select **\"Copy path\"**.  \n",
        "- Paste that path between the quotes in the `input_docx_file` variable below keeping `.docx` extension.\n",
        "\n",
        "You have to also name the output `.txt` files.\n",
        "\n",
        "⚙️ You can also change the `language` variable to:\n",
        "- `'uk'` for **Ukrainian**\n",
        "- `'ru'` for **Russian**\n",
        "\n",
        "> **Note:** Files uploaded this way will be removed once the session ends. You can ignore any temporary file warnings from Colab.\n"
      ],
      "metadata": {
        "id": "u_bvW_K64vdb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Insert between quotes path to your .docx file\n",
        "input_docx_file = '2022-03-16_CUH_U-001.docx'\n",
        "# Type the name of the output files (with .txt extension) that will be created after execution of the following code cell\n",
        "output_persons_file = 't1.txt'\n",
        "output_locations_file = 't2.txt'\n",
        "# Change 'uk' to 'ru' if you want to process Russian text and vice versa\n",
        "language = 'uk'"
      ],
      "metadata": {
        "id": "3NKexRfz5CYu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 🔄 Step 2: Load and Preprocess Document Text\n",
        "\n",
        "We'll read the `.docx` file and append a lowercased version of the text to help improve NER recognition.\n"
      ],
      "metadata": {
        "id": "lnJwzjfOO6d-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from docx import Document\n",
        "import stanza\n",
        "import torch\n",
        "import numpy as np\n",
        "\n",
        "def read_docx(file_path):\n",
        "    doc = Document(file_path)\n",
        "    return '\\n'.join(para.text for para in doc.paragraphs)\n",
        "\n",
        "doc_text = read_docx(input_docx_file).strip()\n",
        "doc_text += '\\n' + doc_text.lower()\n",
        "stanza.download(language)\n",
        "nlp = stanza.Pipeline(language)\n",
        "\n",
        "doc = nlp(doc_text)\n",
        "\n",
        "person_tag = 'PERS' if language == 'uk' else 'PER'\n",
        "\n",
        "unique_persons = {ent.text for ent in doc.ents if ent.type == person_tag}\n",
        "unique_locations = {ent.text for ent in doc.ents if ent.type == 'LOC'}\n",
        "with open(output_persons_file, 'w', encoding='utf-8') as persons_file:\n",
        "    for person in sorted(unique_persons):\n",
        "        persons_file.write(person + '\\n')\n",
        "\n",
        "with open(output_locations_file, 'w', encoding='utf-8') as locations_file:\n",
        "    for location in sorted(unique_locations):\n",
        "        locations_file.write(location + '\\n')\n",
        "\n",
        "print(f\"Unique persons written to: {output_persons_file}\")\n",
        "print(f\"Unique locations written to: {output_locations_file}\")\n"
      ],
      "metadata": {
        "id": "NDWHNOGVO7-B"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}